# LLM Provider configuration
llm:
  # Options: anthropic, openai, ollama
  provider: "ollama"

  # API key (use env var format for security)
  # Will use ANTHROPIC_API_KEY, OPENAI_API_KEY, or OLLAMA_ENDPOINT env vars if not set
  api_key: ""

  # Model selection
  # Anthropic: claude-sonnet-4-20250514, claude-opus-4-6-20250514, claude-haiku-4-20250508
  # OpenAI: gpt-4o, gpt-4-turbo, gpt-3.5-turbo
  # Ollama: llama3.1:8b, llama3, mistral, codellama, deepseek-r1:7b, qwen2.5-coder:1.5b, etc.
  model: "neural-chat:latest"

  # Ollama-specific settings
  endpoint: "http://localhost:11434"

  # Request settings
  max_tokens: 4096
  temperature: 0.7

  # Offline mode: use cached responses when API is unavailable
  offline_mode: false

  # Cache for offline responses
  cache_dir: "data/cache"

# Model configuration (for local PyTorch model - optional)
model:
  d_model: 512
  nhead: 8
  num_layers: 6
  dim_feedforward: 2048
  dropout: 0.1
  max_seq_length: 1024
  weights_path: "models/cyberlab_slm.pt"

# Tokenizer configuration
tokenizer:
  model_path: "models/cyberlab_tokenizer.model"
  vocab_size: 8000

# Parser configuration
parser:
  patterns_file: "config/patterns.yaml"

# Cache configuration
cache:
  file_path: "data/analysis_cache.yaml"
  max_size: 10000

# API configuration
api:
  host: "0.0.0.0"
  port: 8000
  workers: 4
  log_level: "info"

# Security settings
security:
  max_request_size: 10485760  # 10MB
  rate_limit: 100
  allowed_log_types:
    - "nmap"
    - "nikto"
    - "wireshark"
    - "tcpdump"
    - "metasploit"
